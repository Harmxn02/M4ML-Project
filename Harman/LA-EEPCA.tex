\documentclass[11pt, a4paper]{article}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}

\usepackage{url}

\title{Project ``Topic: Eigenvalues and Eigenvectors in Principal Component Analysis'' \\
	{Title} \\ 
	IB3702 Mathematics for Machine Learning}
\author{Harman Singh}
\date{15 November, 2025}

\begin{document}
\maketitle

\section{Introduction}

% r\cite{jolliffe2002pca}

% \textbf{Purpose:} Introduce the motivation and general context of the topic.

% \textbf{Content:}

% \begin{itemize}
% 	\item Machine learning relies heavily on mathematics to model, analyse, and interpret data.
% 	\item High-dimensional datasets are common in modern applications, making dimensionality reduction a crucial step for improving performance and interpretability.
% 	\item Principal Component Analysis (PCA) is one of the most widely used techniques for this purpose.
% 	\item The report focuses on the mathematical foundations of PCA, particularly the role of eigenvalues and eigenvectors in identifying directions of maximum variance.
% 	\item The aim is to connect theoretical concepts from linear algebra to their practical use in machine learning.
% \end{itemize}

Machine learning (ML) relies heavily on mathematics. Luckily, ML is a rather new branch of computer science, which means that it has access to hundreds of years of advancements in the field of mathematics to base its core around, rather than having to discover new mathematical concepts. This means, usually, that the mathematics used in ML is rather straightforward. You build upon centuries worth of linear algebra and calculus to model, analyse and interpret data.

One technique to interpret data, and the topic of this report, is `Principal Component Analysis' (PCA). PCA is a linear dimensionality reduction technique used in exploratory data analysis, with one main purpose {\textemdash} to reduce the dimensionality of a dataset, i.e., to reduce the number of columns (variables) which in turn makes the underlying dataset easier to process by computers. It does this by transforming the data into a new set of variables, the principal components (PCs), which are uncorrelated and ordered by the amount of variance they capture from the original data. The first principal component captures the most variance, the second captures the second most, and so on. This transformation is achieved through the mathematical concepts of eigenvalues and eigenvectors.

The aim of this report is to explore a new field of mathematics and gain knowledge in it. We will connect the theoretical concepts of eigenvalues and eigenvectors from linear algebra to their practical use case in PCA\@.

\pagebreak
\section{Preliminaries}

% \textbf{Purpose:} Provide the mathematical background required to understand PCA

% \textbf{Content:}

% \begin{itemize}
% 	\item Review of key linear algebra concepts relevant to PCA\@:
% 	\begin{itemize}
% 		\item A square matrix \(A\) acting on a vector \(\mathbf{v}\) satisfies the eigenvalue equation \(A\mathbf{v} = \lambda\mathbf{v}\), where \(\lambda\) is an eigenvalue and \(\mathbf{v}\) is the corresponding eigenvector.
% 		\item Geometrically, eigenvectors indicate directions that remain unchanged under the transformation \(A\), and eigenvalues represent the scaling factor along those directions.
% 	\end{itemize}
% 	\item Definition of the covariance matrix as a measure of how variables in a dataset vary together:
% 	\[
% 	\Sigma = \frac{1}{n-1}{(X - \bar{X})}^T(X - \bar{X})
% 	\]
% 	where \(X\) is the data matrix and \(\bar{X}\) is the mean vector.
% 	\item PCA involves the eigen-decomposition of this covariance matrix to determine the principal components.
% 	\item These eigenvectors form a new orthogonal basis for the data space, ordered by decreasing eigenvalues corresponding to decreasing variance captured.
% \end{itemize}



\section{Methods}

% \textbf{Purpose:} Explain how PCA works step-by-step, highlighting the role of eigenvalues and eigenvectors.

% \textbf{Content:}

% \begin{itemize}
% 	\item Describe the goal of PCA (to reduce dimensionality while retaining as much information (variance) as possible)
% 	\item Outline the PCA algorithm briefly:
% 	\begin{enumerate}
% 		\item Standardise the dataset (zero mean, unit variance).
% 		\item Compute the covariance matrix of the data.
% 		\item Perform eigen-decomposition of the covariance matrix.
% 		\item Sort eigenvalues (descending) and select the top \(k\) eigenvectors (principal components).
% 		\item Project data onto the new subspace.
% 	\end{enumerate}
% 	\item Explain the mathematical interpretation:
% 	\begin{itemize}
% 		\item Eigenvectors represent directions of maximum variance.
% 		\item Eigenvalues indicate how much variance is captured along each direction.
% 	\end{itemize}
% 	\item Optionally, relate this to Singular Value Decomposition (SVD), which can also be used for PCA
% \end{itemize}


\section{Numerical Examples}

% \textbf{Purpose:} Provide a small illustrative example to demonstrate PCA in practice.

% \textbf{Content:}

% \begin{itemize}
% 	\item Use a simple 2D or 3D dataset (e.g., two correlated variables like height and weight).
% 	\item Show (conceptually, not full code) how to:
% 	\begin{enumerate}
% 		\item Compute the covariance matrix.
% 		\item Find its eigenvalues and eigenvectors.
% 		\item Project data onto the first principal component.
% 	\end{enumerate}
% 	\item Briefly interpret results:
% 	\begin{itemize}
% 		\item First principal component captures most of the variance.
% 		\item Dimensionality reduced from 2D to 1D (example result).
% 	\end{itemize}
% 	\item You could include a small diagram or table if allowed.
% \end{itemize}



\section{Collaboration}

% \textbf{Purpose:} Explain how the group collaborated and divided tasks.

% \textbf{Content:}

% \begin{itemize}
% 	\item Describe who worked on what:
% 	\begin{itemize}
% 		\item Tobias handled \ldots
% 		\item Harman handled \ldots
% 	\end{itemize}
% 	\item Mention the communication method (e.g., shared documents, group meetings, GitHub, etc.).
% 	\item Reflect briefly on teamwork effectiveness.
% \end{itemize}


\section{Reflection}

% \textbf{Purpose:} Personal reflections on the learning experience.

% \textbf{Content for person:}

\subsection{Student a: Tobias Hungwe}

Text\ldots

\subsection{Student b: Harman Singh}

% \begin{itemize}
% 	\item Reflect on the practical side: implementing PCA, interpreting eigenvalues, and visualising results.
% 	\item Mention insights on how mathematics enables dimensionality reduction and data interpretation.
% 	\item Optionally note any challenges
% 	\begin{itemize}
% 		\item understanding covariance or eigen-decomposition intuitively
% 	\end{itemize}
% \end{itemize}




\pagebreak
\bibliographystyle{plain}
\bibliography{references}
\end{document}