% chktex-file 44
\subsection{Introduction}

% \textbf{Purpose:} Introduce the motivation and general context of the topic.

% \textbf{Content:}

% \begin{itemize}
% 	\item Machine learning relies heavily on mathematics to model, analyse, and interpret data.
% 	\item High-dimensional datasets are common in modern applications, making dimensionality reduction a crucial step for improving performance and interpretability.
% 	\item Principal Component Analysis (PCA) is one of the most widely used techniques for this purpose.
% 	\item The report focuses on the mathematical foundations of PCA, particularly the role of eigenvalues and eigenvectors in identifying directions of maximum variance.
% 	\item The aim is to connect theoretical concepts from linear algebra to their practical use in machine learning.
% \end{itemize}

Machine learning (ML) relies heavily on mathematics. Luckily, ML is a rather new branch of computer science, which means that it has access to hundreds of years of advancements in the field of mathematics to base its core around, rather than having to discover new mathematical concepts. This means, usually, that the mathematics used in ML is rather straightforward. You build upon centuries worth of linear algebra and calculus to model, analyse and interpret data.

One technique to interpret data, and the topic of this report, is `Principal Component Analysis' (PCA). PCA is a linear dimensionality reduction technique used in exploratory data analysis, with one main purpose {\textemdash} to reduce the dimensionality of a dataset, i.e., to reduce the number of columns (variables) which in turn makes the underlying dataset easier to process by computers. It does this by transforming the data into a new set of variables, the principal components (PCs), which are uncorrelated and ordered by the amount of variance they capture from the original data. The first principal component captures the most variance, the second captures the second most, and so on. This transformation is achieved through the mathematical concepts of eigenvalues and eigenvectors.

The aim of this report is to explore a new field of mathematics and gain knowledge in it. We will connect the theoretical concepts of eigenvalues and eigenvectors from linear algebra to their practical use case in PCA\@.



% \pagebreak
% \tableofcontents


\pagebreak
\subsection{Preliminaries}

\subsubsection{Vectors}

You can visualise a column in a dataset as a vector in a high-dimensional space. Each row in the dataset corresponds to a component of the vector, so a dataset with \(n\) observations can be represented as a vector in an \(n\)-dimensional space.

\[
	\begin{tabular}{|c|c|}
		\hline
		$\text{x}$ & $\text{y}$ \\
		\hline
		0.41 & 0.36 \\
		0.24 & 0.09 \\
		0.77 & 0.66 \\
		\hline
	\end{tabular} \rightarrow
\quad
	\mathbf{v_x} = \begin{bmatrix}
		0.41 \\
		0.24 \\
		0.77
	\end{bmatrix}, \quad
	\mathbf{v_y} = \begin{bmatrix}
		0.36 \\
		0.09 \\
		0.66
	\end{bmatrix}
\]

Here, the columns \texttt{x} and \texttt{y} from the dataset are represented as vectors \(\mathbf{v_x}\) and \(\mathbf{v_y}\) in a 3-dimensional space.


\subsubsection{Linear transformations}

A linear transformation takes a vector as input and produces another vector as output, while maintaining the structure of the vector space.
For example, a linear transformation is represented by a matrix \(A\):
\[A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}\]

When this matrix transforms the vector \(\mathbf{v} = \begin{bmatrix}1 \\ 1\end{bmatrix}\), the result is:
\[A\mathbf{v} = 
		\begin{bmatrix}
			2 & 0 \\
			0 & 3
		\end{bmatrix}
		\begin{bmatrix}
			1 \\
			1
		\end{bmatrix} =
		\begin{bmatrix}2 \\
			3
		\end{bmatrix}
\]

This transformation scales the first component of the vector by 2 and the second component by 3.

\subsubsection{Eigenvectors and Eigenvalues}

An eigenvector is a special type of vector where, when a linear transformation is applied to it, only the scale of the vector changes but not its direction. The eigenvalue is the factor by which the eigenvector scaled.

It is easier explained with a visualiation. In the figure below, the vector $v$ retained the same direction after a linear transformation, unlike the vector $w$. This means that $v$ is an eigenvector and the distance between $v$ and $Av$ is its eigenvalue

The matrix and vectors used in the figure are:
\[
	A = \begin{bmatrix}
		2 & 2 \\
		-4 & 8
	\end{bmatrix}, \quad
	v = \begin{bmatrix}
		1 \\
		1
	\end{bmatrix}, \quad
	w = \begin{bmatrix}
		2 \\
		1
	\end{bmatrix}
\]

If we calculate \(Av\) and \(Aw\) we can confirm that the vector \(v\) was simply scaled by a factor of 4, and that \(w\) changed direction.

\[
	Av = \begin{bmatrix}
		2 & 2 \\
		-4 & 8
	\end{bmatrix}
	\begin{bmatrix}
		1 \\
		1
	\end{bmatrix} =
	\begin{bmatrix}
		4 \\
		4
	\end{bmatrix} = 4 \cdot
	\begin{bmatrix}
		1 \\
		1
	\end{bmatrix}
\]

\[
	Aw = \begin{bmatrix}
		2 & 2 \\
		-4 & 8
	\end{bmatrix}
	\begin{bmatrix}
		2 \\
		1
	\end{bmatrix} =
	\begin{bmatrix}
		6 \\
		0
	\end{bmatrix}
\]


\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{images/LINEAR_ALGEBRA/eigenvector-visualisation.png}
	\caption{Visualization of eigenvectors and eigenvalues under a linear transformation~\cite{Libretexts_2023}.}
\end{figure}

\subsubsection{Eigen-decomposition}
Eigen-decomposition is a method used to break down a square matrix into its eigenvalues and eigenvectors. For a square matrix \(A\), the eigen-decomposition can be expressed as:
\[
	A = V \Lambda V^{-1}
\]
where \(V\) is a matrix whose columns are the eigenvectors of \(A\), and \(\Lambda\) is a diagonal matrix containing the corresponding eigenvalues.

We take the example matrix \(A\) from the previous subsection:
\[
	A = \begin{bmatrix}
		2 & 2 \\
		-4 & 8
	\end{bmatrix}
\]


\underline{For $\lambda = 4$}

\[
(A - 4I) v = 0 \implies \begin{bmatrix}
	-2 & 2 \\
	-4 & 4
\end{bmatrix}
\begin{bmatrix}
	x \\
	y
\end{bmatrix} = 0 \implies -2x + 2y = 0 \implies y = x
\]

\underline{For $\lambda = 6$}

\[
(A - 6I) v = 0 \implies \begin{bmatrix}
	-4 & 2 \\
	-4 & 2
\end{bmatrix}
\begin{bmatrix}
	x \\
	y
\end{bmatrix} = 0 \implies -4x + 2y = 0 \implies y = 2x
\]

So the eigenvectors corresponding to the eigenvalues \(\lambda_1 = 4\) and \(\lambda_2 = 6\) are:

\[
	v_1 = \begin{bmatrix}
		1 \\
		1
	\end{bmatrix}, \quad
	v_2 = \begin{bmatrix}
		1 \\
		2
	\end{bmatrix}
\]

\subsubsection{Covariance matrix}

Previously we explained that PCA transforms data into a new set of variables, the principal components, which are uncorrelated and ordered by the amount of variance they capture from the original data.
To find these principal components, PCA uses the eigen-decomposition of the covariance matrix (a measure of how variables in a dataset vary together). 

The covariance matrix $S$ is defined as:
\[
	S = \frac{1}{n-1}{(X - \bar{X})}^T(X - \bar{X})
\]
where \(X\) is the data matrix and \(\bar{X}\) is the mean vector.


% \textbf{Purpose:} Provide the mathematical background required to understand PCA

% \textbf{Content:}

% \begin{itemize}
% 	\item Review of key linear algebra concepts relevant to PCA\@:
% 	\begin{itemize}
% 		\item A square matrix \(A\) acting on a vector \(\mathbf{v}\) satisfies the eigenvalue equation \(A\mathbf{v} = \lambda\mathbf{v}\), where \(\lambda\) is an eigenvalue and \(\mathbf{v}\) is the corresponding eigenvector.
% 		\item Geometrically, eigenvectors indicate directions that remain unchanged under the transformation \(A\), and eigenvalues represent the scaling factor along those directions.
% 	\end{itemize}
% 	\item Definition of the covariance matrix as a measure of how variables in a dataset vary together:
% 	\[
% 	\Sigma = \frac{1}{n-1}{(X - \bar{X})}^T(X - \bar{X})
% 	\]
% 	where \(X\) is the data matrix and \(\bar{X}\) is the mean vector.
% 	\item PCA involves the eigen-decomposition of this covariance matrix to determine the principal components.
% 	\item These eigenvectors form a new orthogonal basis for the data space, ordered by decreasing eigenvalues corresponding to decreasing variance captured.
% \end{itemize}



\subsection{Methods}

% \textbf{Purpose:} Explain how PCA works step-by-step, highlighting the role of eigenvalues and eigenvectors.

% \textbf{Content:}

% \begin{itemize}
% 	\item Describe the goal of PCA (to reduce dimensionality while retaining as much information (variance) as possible)
% 	\item Outline the PCA algorithm briefly:
% 	\begin{enumerate}
% 		\item Standardise the dataset (zero mean, unit variance).
% 		\item Compute the covariance matrix of the data.
% 		\item Perform eigen-decomposition of the covariance matrix.
% 		\item Sort eigenvalues (descending) and select the top \(k\) eigenvectors (principal components).
% 		\item Project data onto the new subspace.
% 	\end{enumerate}
% 	\item Explain the mathematical interpretation:
% 	\begin{itemize}
% 		\item Eigenvectors represent directions of maximum variance.
% 		\item Eigenvalues indicate how much variance is captured along each direction.
% 	\end{itemize}
% 	\item Optionally, relate this to Singular Value Decomposition (SVD), which can also be used for PCA
% \end{itemize}

\subsubsection{The PCA algorithm}


\textbf{1. Standardise the dataset}

Subtract the mean and scale by the standard deviation for each feature to make sure that each feature contributes equally. For this we use the z-score transformation formula, but adapted for each feature \(j\):

\[z_{ij} = \frac{x_{ij}-\mu_{j}}{\sigma_{j}}\]

with $\mu_{j}$ being the mean of feature \(j\) and $\sigma_{j}$ being the standard deviation of feature \(j\).



\textbf{2. Calculate the covariance matrix}
The covariance matrix \(S\) is calculated as:
\[S = \frac{1}{n-1} {Z}^T Z\]

with \(Z\) being the standardised data matrix.


\textbf{3. Perform eigen-decomposition on the covariance matrix}
We perform eigen-decomposition on the covariance matrix \(S\):
\[S = V \Lambda V^{-1}\]
with \(V\) being the matrix of eigenvectors and \(\Lambda\) is the diagonal matrix of eigenvalues.


\textbf{4. Sort eigenvalues and select top \(k\) eigenvectors}
\[ \boxed{ \lambda_1 \geq \lambda_2} \geq \lambda_3 \geq \cdots \geq \lambda_p\]

PCA is not a lossless algorithm, meaning that we do lose some information when we reduce the dimensionality of the data. However, by selecting the top \(k\) eigenvectors (principal components), we retain the directions that capture the most variance in the data, and this usually gives a pretty good approximation of the original data.




\subsection{Numerical Examples}

% \textbf{Purpose:} Provide a small illustrative example to demonstrate PCA in practice.

% \textbf{Content:}

% \begin{itemize}
% 	\item Use a simple 2D or 3D dataset (e.g., two correlated variables like height and weight).
% 	\item Show (conceptually, not full code) how to:
% 	\begin{enumerate}
% 		\item Compute the covariance matrix.
% 		\item Find its eigenvalues and eigenvectors.
% 		\item Project data onto the first principal component.
% 	\end{enumerate}
% 	\item Briefly interpret results:
% 	\begin{itemize}
% 		\item First principal component captures most of the variance.
% 		\item Dimensionality reduced from 2D to 1D (example result).
% 	\end{itemize}
% 	\item You could include a small diagram or table if allowed.
% \end{itemize}
