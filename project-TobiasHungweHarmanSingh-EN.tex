\documentclass[11pt, a4paper]{article}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{amssymb}

\usepackage{float}

\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{references.bib}  % your .bib file

\title{
	Project ``Topic: Eigenvalues and Eigenvectors in PCA\@; Derivatives in Neural Networks (NN)'' \\
	IB3702 Mathematics for Machine Learning}
\author{Tobias Hungwe \and Harman Singh}
\date{15 november, 2025}

\begin{document}
\maketitle

\pagebreak
\tableofcontents
\pagebreak

\section{Eigenvalues and Eigenvectors in Principal Component Analysis (PCA)}
\input{LA-EEPCA.tex}

\section{Derivatives in Neural Networks (NN)}
\input{C-DNN.tex}




\pagebreak
\section{Collaboration}
\subsection{Topic choice}
We went through the document with all possible topics, highlighted the ones we found interesting, and then discussed which 2 we liked the most for each of the two parts.

Initially we also had interest for ``2.1.3 Topic 3: Hessian Matrix and Second-Order Optimization in Machine Learning'', but we decided that Tobias handle the ``Derivative in Neural Networks'' topic since it was already covered in our bachelor's degree.


\subsection{Code sharing}
We used a shared GitHub repository to store all files for the project on. We used 3 LaTeX files, one for each of us, and a third that combines both parts and the collaboration/reflection parts.

\subsection{Communication}
Throughout the project we mainly communicated through Discord, before, during, and after the course meetings.


\section{Reflection}

\subsection{Tobias Hungwe}
In this project I finally saw how calculus actually drives learning in a neural network. The derivative became the concrete “rate of change” of the loss when a weight is adjusted, and by repeating many small updates I could see the model improve step by step. Linking these calculations to a simple numerical example and the Wine experiment helped me connect the formulas, the code, and the decreasing loss curve in a very tangible way.

\subsection{Harman Singh}

I had explored PCA already briefly before, but was not aware of the mathematics behind it. It was interesting to see that the what we learned in the course was directly connected to PCA\@. It's a technique that I have used in Python code before to do dimensionality reduction, and now after this project I understand better what the algorithm is actually doing. I ``opened the black box'', so to say.



\pagebreak
\printbibliography{}

\end{document}  